{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                              WEB-SCRAPPING-ASSIGNMENT - 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1: Write a python program to scrape data for “Data Analyst” Job position in“Bangalore” location. You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the url:https://www.naukri.com/data-analyst-jobs-in-bangalore?k=data%20analyst&l=bangalore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_Title</th>\n",
       "      <th>Job_Location</th>\n",
       "      <th>Company_Name</th>\n",
       "      <th>Experience_Required</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Immediate opening For Data Scientist/Data Analyst</td>\n",
       "      <td>Chennai, Pune, Bengaluru, Hyderabad</td>\n",
       "      <td>CAIA-Center For Artificial Intelligence &amp; Adva...</td>\n",
       "      <td>0-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Chennai, Delhi NCR, Bengaluru</td>\n",
       "      <td>Hk solutions</td>\n",
       "      <td>0-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Intern - DFM Data Analyst</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>GLOBALFOUNDRIES Engineering Private Limited</td>\n",
       "      <td>0-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hiring Data Analysts on Contract Third party p...</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Flipkart Internet Private Limited</td>\n",
       "      <td>2-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Reliability Data Analyst</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Alstom Transport India Ltd.</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Myntra</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Ladder of changes</td>\n",
       "      <td>0-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Analyst / Business Analyst</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Altisource</td>\n",
       "      <td>1-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Hiring For  Data Analyst RE) - Bangalore</td>\n",
       "      <td>Bengaluru(Bellandur)</td>\n",
       "      <td>TELEPERFORMANCE GLOBAL SERVICES</td>\n",
       "      <td>2-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Specialist I / II - Data Analyst</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Philips India Limited</td>\n",
       "      <td>5-7 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Job_Title  \\\n",
       "0  Immediate opening For Data Scientist/Data Analyst   \n",
       "1                                       Data Analyst   \n",
       "2                          Intern - DFM Data Analyst   \n",
       "3  Hiring Data Analysts on Contract Third party p...   \n",
       "4                           Reliability Data Analyst   \n",
       "5                                Senior Data Analyst   \n",
       "6                                       Data Analyst   \n",
       "7                    Data Analyst / Business Analyst   \n",
       "8           Hiring For  Data Analyst RE) - Bangalore   \n",
       "9                   Specialist I / II - Data Analyst   \n",
       "\n",
       "                          Job_Location  \\\n",
       "0  Chennai, Pune, Bengaluru, Hyderabad   \n",
       "1        Chennai, Delhi NCR, Bengaluru   \n",
       "2                            Bengaluru   \n",
       "3                            Bengaluru   \n",
       "4                            Bengaluru   \n",
       "5                            Bengaluru   \n",
       "6                            Bengaluru   \n",
       "7                            Bengaluru   \n",
       "8                 Bengaluru(Bellandur)   \n",
       "9                            Bengaluru   \n",
       "\n",
       "                                        Company_Name Experience_Required  \n",
       "0  CAIA-Center For Artificial Intelligence & Adva...             0-3 Yrs  \n",
       "1                                       Hk solutions             0-3 Yrs  \n",
       "2        GLOBALFOUNDRIES Engineering Private Limited             0-5 Yrs  \n",
       "3                  Flipkart Internet Private Limited             2-6 Yrs  \n",
       "4                        Alstom Transport India Ltd.             3-8 Yrs  \n",
       "5                                             Myntra             2-7 Yrs  \n",
       "6                                  Ladder of changes             0-5 Yrs  \n",
       "7                                         Altisource             1-6 Yrs  \n",
       "8                    TELEPERFORMANCE GLOBAL SERVICES             2-6 Yrs  \n",
       "9                              Philips India Limited             5-7 Yrs  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure webdriver to use Chrome Browser\n",
    "\n",
    "def scraping():\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    from selenium import webdriver\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    import requests\n",
    "    import urllib.request\n",
    "    from urllib.request import urlopen\n",
    "    \n",
    "    url=input(\"Enter the url:\")\n",
    "    driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "    driver.get(url)\n",
    "    \n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content)\n",
    "    \n",
    "    title = soup.find_all('div', class_ = 'info fleft')\n",
    "    Job_Title = []\n",
    "    for i in title:\n",
    "        Job_Title.append(i.a.text)\n",
    "    \n",
    "    location = soup.find_all('li', class_ = 'fleft grey-text br2 placeHolderLi location')\n",
    "    Job_Location = []\n",
    "    for i in location:\n",
    "        Job_Location.append(i.text)\n",
    "    \n",
    "    \n",
    "    coname = soup.find_all('div', class_ = 'mt-7 companyInfo subheading lh16')\n",
    "    Company_Name = []\n",
    "    for i in coname:\n",
    "        Company_Name.append(i.a.text)\n",
    "    \n",
    "    experience = soup.find_all('li', class_ = 'fleft grey-text br2 placeHolderLi experience')\n",
    "    Experience_Required = []\n",
    "    for i in experience:\n",
    "        Experience_Required.append(i.text)\n",
    "    \n",
    "    DataAnalyst_Banglore = pd.DataFrame({'Job_Title' : Job_Title[0:10], 'Job_Location' : Job_Location[0:10], 'Company_Name' : Company_Name[0:10], 'Experience_Required' : Experience_Required[0:10] })\n",
    "    return DataAnalyst_Banglore\n",
    "\n",
    "scraping()\n",
    "#Enter the url:https://www.naukri.com/data-analyst-jobs-in-bangalore?k=data%20analyst&l=bangalore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2: Write a python program to scrape data for “Data Scientist” Job position in“Bangalore” location. You have to scrape the job-title, job-location,company_name, full job-description. You have to scrape first 10 jobs data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the url:https://www.naukri.com/data-scientist-jobs-in-bangalore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_Title</th>\n",
       "      <th>Job_Location</th>\n",
       "      <th>Company_Name</th>\n",
       "      <th>Experience_Required</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Immediate opening For Data Scientist/Data Analyst</td>\n",
       "      <td>Chennai, Pune, Bengaluru, Hyderabad</td>\n",
       "      <td>CAIA-Center For Artificial Intelligence &amp; Adva...</td>\n",
       "      <td>0-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Flipkart</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist- Computer Vision &amp; Image Proces...</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>24/7 Customer</td>\n",
       "      <td>0-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Groupon Shared Services</td>\n",
       "      <td>4-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sr. Data Scientist</td>\n",
       "      <td>Chennai, Bengaluru</td>\n",
       "      <td>AVE-Promagne</td>\n",
       "      <td>6-11 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Senior Data Scientist- Bangalore/ Chennai</td>\n",
       "      <td>Chennai, Bengaluru</td>\n",
       "      <td>RANDSTAD INDIA PVT LTD</td>\n",
       "      <td>8-13 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>ORMAE LLP</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sr. Data Scientist</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>NetApp</td>\n",
       "      <td>10-20 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AI Resident - Data Scientist</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Shell India Markets Private Limited</td>\n",
       "      <td>3-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data scientist</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>IBM India Pvt. Limited</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Job_Title  \\\n",
       "0  Immediate opening For Data Scientist/Data Analyst   \n",
       "1                              Senior Data Scientist   \n",
       "2  Data Scientist- Computer Vision & Image Proces...   \n",
       "3                              Senior Data Scientist   \n",
       "4                                 Sr. Data Scientist   \n",
       "5          Senior Data Scientist- Bangalore/ Chennai   \n",
       "6                              Senior Data Scientist   \n",
       "7                                 Sr. Data Scientist   \n",
       "8                       AI Resident - Data Scientist   \n",
       "9                                     Data scientist   \n",
       "\n",
       "                          Job_Location  \\\n",
       "0  Chennai, Pune, Bengaluru, Hyderabad   \n",
       "1                            Bengaluru   \n",
       "2                            Bengaluru   \n",
       "3                            Bengaluru   \n",
       "4                   Chennai, Bengaluru   \n",
       "5                   Chennai, Bengaluru   \n",
       "6                            Bengaluru   \n",
       "7                            Bengaluru   \n",
       "8                            Bengaluru   \n",
       "9                            Bengaluru   \n",
       "\n",
       "                                        Company_Name Experience_Required  \n",
       "0  CAIA-Center For Artificial Intelligence & Adva...             0-3 Yrs  \n",
       "1                                           Flipkart             2-7 Yrs  \n",
       "2                                      24/7 Customer             0-6 Yrs  \n",
       "3                            Groupon Shared Services             4-8 Yrs  \n",
       "4                                      AVE-Promagne             6-11 Yrs  \n",
       "5                             RANDSTAD INDIA PVT LTD            8-13 Yrs  \n",
       "6                                          ORMAE LLP            5-10 Yrs  \n",
       "7                                             NetApp           10-20 Yrs  \n",
       "8                Shell India Markets Private Limited             3-5 Yrs  \n",
       "9                             IBM India Pvt. Limited             2-7 Yrs  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def scraping():\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    from selenium import webdriver\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    import requests\n",
    "    import urllib.request\n",
    "    from urllib.request import urlopen\n",
    "    \n",
    "    url=input(\"Enter the url:\")\n",
    "    driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "    driver.get(url)\n",
    "    \n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content)\n",
    "    \n",
    "    title = soup.find_all('div', class_ = 'info fleft')\n",
    "    Job_Title = []\n",
    "    for i in title:\n",
    "        Job_Title.append(i.a.text)\n",
    "    \n",
    "    location = soup.find_all('li', class_ = 'fleft grey-text br2 placeHolderLi location')\n",
    "    Job_Location = []\n",
    "    for i in location:\n",
    "        Job_Location.append(i.text)\n",
    "    \n",
    "    \n",
    "    coname = soup.find_all('div', class_ = 'mt-7 companyInfo subheading lh16')\n",
    "    Company_Name = []\n",
    "    for i in coname:\n",
    "        Company_Name.append(i.a.text)\n",
    "    \n",
    "    experience = soup.find_all('li', class_ = 'fleft grey-text br2 placeHolderLi experience')\n",
    "    Experience_Required = []\n",
    "    for i in experience:\n",
    "        Experience_Required.append(i.text)\n",
    "    \n",
    "    DataScientist_Banglore = pd.DataFrame({'Job_Title' : Job_Title[0:10], 'Job_Location' : Job_Location[0:10], 'Company_Name' : Company_Name[0:10], 'Experience_Required' : Experience_Required[0:10] })\n",
    "    return DataScientist_Banglore\n",
    "\n",
    "scraping()\n",
    "#https://www.naukri.com/data-scientist-jobs-in-bangalore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3: In this question you have to scrape data using the filters available on the webpage as shown below:\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "You have to use the location and salary filter.\n",
    "You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "You have to scrape the job-title, job-location, company_name,\n",
    "experience_required.\n",
    "The location filter to be used is “Delhi/NCR”\n",
    "The salary filter to be used is “3-6” lakhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the url:https://www.naukri.com/data-scientist-jobs-in-delhi-ncr?cityType=25.9.31&industryTypeId=34&ctcFilter=3to6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_Title</th>\n",
       "      <th>Job_Location</th>\n",
       "      <th>Company_Name</th>\n",
       "      <th>Experience_Required</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Business Analyst - Data Scientist</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>HyreFox Consultants Pvt Ltd</td>\n",
       "      <td>3-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Analyst - Data Scientist</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>HyreFox Consultants Pvt Ltd</td>\n",
       "      <td>1-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>T &amp; A Solutions</td>\n",
       "      <td>1-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>itForte Staffing Services Private Ltd.</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Noida</td>\n",
       "      <td>Sumeru Verde Technologies</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Analyst/Scientist</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>itForte Staffing Services Private Ltd.</td>\n",
       "      <td>3-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Chennai, Delhi NCR, Bengaluru</td>\n",
       "      <td>Hk solutions</td>\n",
       "      <td>0-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Reporting and Data Analyst</td>\n",
       "      <td>Noida</td>\n",
       "      <td>Finejobs Consultant Private Limited</td>\n",
       "      <td>0-1 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Chennai, Pune, Delhi NCR, Bengaluru</td>\n",
       "      <td>JoulestoWatts Business Solutions Pvt Ltd</td>\n",
       "      <td>4-9 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>VTalentGlobal</td>\n",
       "      <td>3-5 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Job_Title                         Job_Location  \\\n",
       "0  Business Analyst - Data Scientist                              Gurgaon   \n",
       "1           Analyst - Data Scientist                              Gurgaon   \n",
       "2                     Data Scientist                              Gurgaon   \n",
       "3                     Data Scientist                              Gurgaon   \n",
       "4                     Data Scientist                                Noida   \n",
       "5             Data Analyst/Scientist                              Gurgaon   \n",
       "6                       Data Analyst        Chennai, Delhi NCR, Bengaluru   \n",
       "7         Reporting and Data Analyst                                Noida   \n",
       "8                       Data Analyst  Chennai, Pune, Delhi NCR, Bengaluru   \n",
       "9                Senior Data Analyst                                Delhi   \n",
       "\n",
       "                               Company_Name Experience_Required  \n",
       "0               HyreFox Consultants Pvt Ltd             3-5 Yrs  \n",
       "1               HyreFox Consultants Pvt Ltd             1-3 Yrs  \n",
       "2                           T & A Solutions             1-6 Yrs  \n",
       "3    itForte Staffing Services Private Ltd.             3-8 Yrs  \n",
       "4                 Sumeru Verde Technologies             2-5 Yrs  \n",
       "5    itForte Staffing Services Private Ltd.             3-7 Yrs  \n",
       "6                              Hk solutions             0-3 Yrs  \n",
       "7       Finejobs Consultant Private Limited             0-1 Yrs  \n",
       "8  JoulestoWatts Business Solutions Pvt Ltd             4-9 Yrs  \n",
       "9                             VTalentGlobal             3-5 Yrs  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def scraping():\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    from selenium import webdriver\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    import requests\n",
    "    import urllib.request\n",
    "    from urllib.request import urlopen\n",
    "    \n",
    "    url=input(\"Enter the url:\")\n",
    "    driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "    driver.get(url)\n",
    "    \n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content)\n",
    "    \n",
    "    title = soup.find_all('div', class_ = 'info fleft')\n",
    "    Job_Title = []\n",
    "    for i in title:\n",
    "        Job_Title.append(i.a.text)\n",
    "    \n",
    "    location = soup.find_all('li', class_ = 'fleft grey-text br2 placeHolderLi location')\n",
    "    Job_Location = []\n",
    "    for i in location:\n",
    "        Job_Location.append(i.text)\n",
    "    \n",
    "    \n",
    "    coname = soup.find_all('div', class_ = 'mt-7 companyInfo subheading lh16')\n",
    "    Company_Name = []\n",
    "    for i in coname:\n",
    "        Company_Name.append(i.a.text)\n",
    "    \n",
    "    experience = soup.find_all('li', class_ = 'fleft grey-text br2 placeHolderLi experience')\n",
    "    Experience_Required = []\n",
    "    for i in experience:\n",
    "        Experience_Required.append(i.text)\n",
    "    \n",
    "    DataScientist_Delhi = pd.DataFrame({'Job_Title' : Job_Title[0:10], 'Job_Location' : Job_Location[0:10], 'Company_Name' : Company_Name[0:10], 'Experience_Required' : Experience_Required[0:10] })\n",
    "    return DataScientist_Delhi\n",
    "\n",
    "scraping()\n",
    "#Enter the url:https://www.naukri.com/data-scientist-jobs-in-delhi-ncr?cityType=25.9.31&industryTypeId=34&ctcFilter=3to6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4: Write a python program to scrape data for first 10 job results for Data scientist Designation in Noida location. You have to scrape company_name, No. of days ago when job was posted, Rating of the company."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This task will be done in following steps:\n",
    "1. first get the webpage https://www.glassdoor.co.in/index.htm\n",
    "2. Enter “Data Scientist” in “Job Title,Keyword,Company” field and enter “Noida”\n",
    "in “location” field.\n",
    "3. Then click the search button. You will land up in the below page:\n",
    "4. Then scrape the data for the first 10 jobs results you get in the above shown\n",
    "page.\n",
    "5. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the url:https://www.glassdoor.co.in/Job/noida-data-scientist-jobs-SRCH_IL.0,5_IC4477468_KO6,20.htm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company_Name</th>\n",
       "      <th>No_Of_Days</th>\n",
       "      <th>Ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xtLytics</td>\n",
       "      <td>1d</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Project YX</td>\n",
       "      <td>24h</td>\n",
       "      <td>3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xtLytics</td>\n",
       "      <td>1d</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EOXS</td>\n",
       "      <td>24h</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ericsson</td>\n",
       "      <td>10d</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Kvantum</td>\n",
       "      <td>1d</td>\n",
       "      <td>4.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>xtLytics</td>\n",
       "      <td>1d</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Transorg Analytics</td>\n",
       "      <td>3d</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Techlive</td>\n",
       "      <td>30d+</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Tidal Vape</td>\n",
       "      <td>1d</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Company_Name No_Of_Days Ratings\n",
       "0            xtLytics         1d       3\n",
       "1          Project YX        24h     3.4\n",
       "2            xtLytics         1d       3\n",
       "3                EOXS        24h     3.8\n",
       "4            Ericsson        10d     3.8\n",
       "5             Kvantum         1d     4.3\n",
       "6            xtLytics         1d       3\n",
       "7  Transorg Analytics         3d     2.1\n",
       "8            Techlive       30d+       5\n",
       "9          Tidal Vape         1d     3.6"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def scraping():\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    from selenium import webdriver\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    import requests\n",
    "    import urllib.request\n",
    "    from urllib.request import urlopen\n",
    "    \n",
    "    url=input(\"Enter the url:\")\n",
    "    driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "    driver.get(url)\n",
    "    \n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content)\n",
    "    \n",
    "    title = soup.find_all('div', class_ = 'jobHeader d-flex justify-content-between align-items-start')\n",
    "    Job_Title = []\n",
    "    for i in title:\n",
    "        Job_Title.append(i.text)\n",
    "    \n",
    "    location = soup.find_all('div', class_ = 'd-flex align-items-end pl-std css-mi55ob')\n",
    "    Job_Location = []\n",
    "    for i in location:\n",
    "        Job_Location.append(i.text)\n",
    "    \n",
    "    \n",
    "    coname = soup.find_all('span', class_ = 'compactStars')\n",
    "    Rating_Name = []\n",
    "    for i in coname:\n",
    "        Rating_Name.append(i.text)\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    DataScientist_Noida = pd.DataFrame({'Company_Name' : Job_Title[0:10], 'No_Of_Days' : Job_Location[0:10], 'Ratings' : Rating_Name[0:10] })\n",
    "    return DataScientist_Noida\n",
    "\n",
    "scraping()\n",
    "#Enter the url:https://www.glassdoor.co.in/Job/noida-data-scientist-jobs-SRCH_IL.0,5_IC4477468_KO6,20.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5: Write a python program to scrape the salary data for Data Scientist designation in Noida location.You have to scrape Company name, Number of salaries, Average salary, Min salary, Max Salary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the url:https://www.glassdoor.co.in/Salaries/new-delhi-data-scientist-salary-SRCH_IL.0,9_IM1083_KO10,24.htm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company_Name</th>\n",
       "      <th>No_Of_Salaries</th>\n",
       "      <th>Avg_Salary</th>\n",
       "      <th>Min_Max_Salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Delhivery</td>\n",
       "      <td>11 salaries</td>\n",
       "      <td>₹ 13,18,563/yr</td>\n",
       "      <td>₹706K₹11,513K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accenture</td>\n",
       "      <td>8 salaries</td>\n",
       "      <td>₹ 9,85,497/yr</td>\n",
       "      <td>₹572K₹1,300K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IBM</td>\n",
       "      <td>7 salaries</td>\n",
       "      <td>₹ 7,53,602/yr</td>\n",
       "      <td>₹581K₹2,704K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UnitedHealth Group</td>\n",
       "      <td>7 salaries</td>\n",
       "      <td>₹ 13,23,634/yr</td>\n",
       "      <td>₹710K₹1,559K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cognizant Technology Solutions</td>\n",
       "      <td>6 salaries</td>\n",
       "      <td>₹ 9,97,979/yr</td>\n",
       "      <td>₹785K₹1,251K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Valiance Solutions</td>\n",
       "      <td>6 salaries</td>\n",
       "      <td>₹ 7,72,507/yr</td>\n",
       "      <td>₹497K₹1,140K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Vidooly Media Tech</td>\n",
       "      <td>6 salaries</td>\n",
       "      <td>₹ 12,689/mo</td>\n",
       "      <td>₹8K₹20K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Analytics Vidhya</td>\n",
       "      <td>6 salaries</td>\n",
       "      <td>₹ 21,215/mo</td>\n",
       "      <td>₹14K₹22K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Tata Consultancy Services</td>\n",
       "      <td>5 salaries</td>\n",
       "      <td>₹ 6,77,498/yr</td>\n",
       "      <td>₹480K₹1,000K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ericsson-Worldwide</td>\n",
       "      <td>5 salaries</td>\n",
       "      <td>₹ 7,34,456/yr</td>\n",
       "      <td>₹460K₹1,598K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Company_Name No_Of_Salaries      Avg_Salary  \\\n",
       "0                       Delhivery    11 salaries  ₹ 13,18,563/yr   \n",
       "1                       Accenture     8 salaries   ₹ 9,85,497/yr   \n",
       "2                             IBM     7 salaries   ₹ 7,53,602/yr   \n",
       "3              UnitedHealth Group     7 salaries  ₹ 13,23,634/yr   \n",
       "4  Cognizant Technology Solutions     6 salaries   ₹ 9,97,979/yr   \n",
       "5              Valiance Solutions     6 salaries   ₹ 7,72,507/yr   \n",
       "6              Vidooly Media Tech     6 salaries     ₹ 12,689/mo   \n",
       "7                Analytics Vidhya     6 salaries     ₹ 21,215/mo   \n",
       "8       Tata Consultancy Services     5 salaries   ₹ 6,77,498/yr   \n",
       "9              Ericsson-Worldwide     5 salaries   ₹ 7,34,456/yr   \n",
       "\n",
       "  Min_Max_Salary  \n",
       "0  ₹706K₹11,513K  \n",
       "1   ₹572K₹1,300K  \n",
       "2   ₹581K₹2,704K  \n",
       "3   ₹710K₹1,559K  \n",
       "4   ₹785K₹1,251K  \n",
       "5   ₹497K₹1,140K  \n",
       "6        ₹8K₹20K  \n",
       "7       ₹14K₹22K  \n",
       "8   ₹480K₹1,000K  \n",
       "9   ₹460K₹1,598K  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def scraping():\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    from selenium import webdriver\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    import requests\n",
    "    import urllib.request\n",
    "    from urllib.request import urlopen\n",
    "    \n",
    "    url=input(\"Enter the url:\")\n",
    "    driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "    driver.get(url)\n",
    "    \n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content)\n",
    "    \n",
    "    title = soup.find_all('p', class_ = 'm-0')\n",
    "    access_dict=[2,8,14,20,26,32,38,44,50,56,62,68,74,80,86,92,98,104,104,106]\n",
    "    Job_Title = []\n",
    "    for i in access_dict:\n",
    "        Job_Title.append(title[i].text)\n",
    "    \n",
    "    location = soup.find_all('p', class_ = 'css-1uyte9r css-1kuy7z7 m-0')\n",
    "    Job_Location = []\n",
    "    for i in location:\n",
    "        Job_Location.append(i.text)\n",
    "    \n",
    "    \n",
    "    coname = soup.find_all('div', class_ = 'col-2 d-none d-md-flex flex-row justify-content-end')\n",
    "    Company_Name = []\n",
    "    for i in coname:\n",
    "        Company_Name.append(i.text)\n",
    "        \n",
    "        \n",
    "    experience = soup.find_all('div', class_ = 'common__RangeBarStyle__values common__flex__justifySpaceBetween common__flex__container')\n",
    "    Experience_Required = []\n",
    "    for i in experience:\n",
    "        Experience_Required.append(i.text)\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    DataScientist_Noida = pd.DataFrame({'Company_Name' : Job_Title[0:10], 'No_Of_Salaries' : Job_Location[0:10], 'Avg_Salary' : Company_Name[0:10], 'Min_Max_Salary' : Experience_Required[0:10] })\n",
    "    return DataScientist_Noida\n",
    "\n",
    "scraping()\n",
    "#https://www.glassdoor.co.in/Salaries/new-delhi-data-scientist-salary-SRCH_IL.0,9_IM1083_KO10,24.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6 : Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:1. Brand 2. Product Description 3. Price 4. Discount %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input url here : To scrape page 1https://www.flipkart.com/search?q=sunglasses&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off\n",
      "Input url here : To scrape page 2https://www.flipkart.com/search?q=sunglasses&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=2\n",
      "Input url here : To scrape page 3https://www.flipkart.com/search?q=sunglasses&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BRAND</th>\n",
       "      <th>PRODUCT_DESCRIPTION</th>\n",
       "      <th>PRICE</th>\n",
       "      <th>DISCOUNT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>United Colors of Benetton</td>\n",
       "      <td>UV Protection Retro Square Sunglasses (57)</td>\n",
       "      <td>₹1,755</td>\n",
       "      <td>[[55% off]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>INSH</td>\n",
       "      <td>UV Protection Round Sunglasses (54)</td>\n",
       "      <td>₹315</td>\n",
       "      <td>[[84% off]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (Free Size)</td>\n",
       "      <td>₹809</td>\n",
       "      <td>[[10% off]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Collet</td>\n",
       "      <td>Gradient, Mirrored, UV Protection Aviator Sung...</td>\n",
       "      <td>₹199</td>\n",
       "      <td>[[86% off]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Elligator</td>\n",
       "      <td>UV Protection Round Sunglasses (54)</td>\n",
       "      <td>₹296</td>\n",
       "      <td>[[88% off]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Gansta</td>\n",
       "      <td>UV Protection Rectangular Sunglasses (56)</td>\n",
       "      <td>₹211</td>\n",
       "      <td>[[78% off]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>ROZZETTA CRAFT</td>\n",
       "      <td>UV Protection, Gradient Rectangular Sunglasses...</td>\n",
       "      <td>₹426</td>\n",
       "      <td>[[85% off]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>blutech</td>\n",
       "      <td>Mirrored Spectacle  Sunglasses (Free Size)</td>\n",
       "      <td>₹140</td>\n",
       "      <td>[[86% off]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Gansta</td>\n",
       "      <td>UV Protection, Gradient Wayfarer Sunglasses (53)</td>\n",
       "      <td>₹269</td>\n",
       "      <td>[[20% off]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Round Sunglasses (52)</td>\n",
       "      <td>₹1,299</td>\n",
       "      <td>[[90% off]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        BRAND  \\\n",
       "0   United Colors of Benetton   \n",
       "1                        INSH   \n",
       "2                    Fastrack   \n",
       "3                      Collet   \n",
       "4                   Elligator   \n",
       "..                        ...   \n",
       "95                     Gansta   \n",
       "96             ROZZETTA CRAFT   \n",
       "97                    blutech   \n",
       "98                     Gansta   \n",
       "99                   Fastrack   \n",
       "\n",
       "                                  PRODUCT_DESCRIPTION   PRICE     DISCOUNT  \n",
       "0          UV Protection Retro Square Sunglasses (57)  ₹1,755  [[55% off]]  \n",
       "1                 UV Protection Round Sunglasses (54)    ₹315  [[84% off]]  \n",
       "2       UV Protection Wayfarer Sunglasses (Free Size)    ₹809  [[10% off]]  \n",
       "3   Gradient, Mirrored, UV Protection Aviator Sung...    ₹199  [[86% off]]  \n",
       "4                 UV Protection Round Sunglasses (54)    ₹296  [[88% off]]  \n",
       "..                                                ...     ...          ...  \n",
       "95          UV Protection Rectangular Sunglasses (56)    ₹211  [[78% off]]  \n",
       "96  UV Protection, Gradient Rectangular Sunglasses...    ₹426  [[85% off]]  \n",
       "97         Mirrored Spectacle  Sunglasses (Free Size)    ₹140  [[86% off]]  \n",
       "98   UV Protection, Gradient Wayfarer Sunglasses (53)    ₹269  [[20% off]]  \n",
       "99                UV Protection Round Sunglasses (52)  ₹1,299  [[90% off]]  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def scrapping():\n",
    "    \n",
    "    # Importing essential libraries\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    from selenium import webdriver\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    import requests\n",
    "    import urllib.request\n",
    "    from urllib.request import urlopen\n",
    "\n",
    "\n",
    "    # Configure the webdriver\n",
    "    driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "    driver.get(input(\"Input url here : To scrape page 1\"))\n",
    "\n",
    "    # Creating soup\n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content)\n",
    "    \n",
    "    brand = soup.find_all('div', class_ = '_2B_pmu')\n",
    "    Brand = []\n",
    "    for i in brand:\n",
    "        Brand.append(i.text[0:40])\n",
    "\n",
    "    # Scraping description of the product\n",
    "    description = soup.find_all('a', class_ = '_2mylT6')\n",
    "    Product_Description = []\n",
    "    for i in description:\n",
    "        Product_Description.append(i.text)\n",
    "\n",
    "    # Scraping price of the product\n",
    "    price = soup.find_all('div', class_ = '_1vC4OE')\n",
    "    Price = []\n",
    "    for i in price:\n",
    "        Price.append(i.text[0:40])\n",
    "\n",
    "    # Scraping discount on the product\n",
    "    discount = soup.find_all('div', class_ = 'VGWI6T')\n",
    "    Discount = []\n",
    "    for i in discount:\n",
    "        Discount.append(i)\n",
    "\n",
    "    \n",
    "    # Scaping next page for more results\n",
    "    driver.get(input(\"Input url here : To scrape page 2\"))\n",
    "\n",
    "    # Creating soup\n",
    "    content = driver.page_source\n",
    "    soup1 = BeautifulSoup(content)\n",
    "    \n",
    "    # Scraping brand of the products - page 2\n",
    "    brand1 = soup1.find_all('div', class_ = '_2B_pmu')\n",
    "    Brand1 = []\n",
    "    for i in brand1:\n",
    "        Brand1.append(i.text[0:40])\n",
    "\n",
    "    # Sctraping description of the products - page 2\n",
    "    description1 = soup1.find_all('a', class_ = '_2mylT6')\n",
    "    Product_Description1 = []\n",
    "    for i in description1:\n",
    "        Product_Description1.append(i.text)\n",
    "\n",
    "    # Scraping price of the products - page 2\n",
    "    price1 = soup1.find_all('div', class_ = '_1vC4OE')\n",
    "    Price1 = []\n",
    "    for i in price1:\n",
    "        Price1.append(i.text[0:40])\n",
    "\n",
    "    # Scraping discount on the products - page 2\n",
    "    discount1 = soup1.find_all('div', class_ = 'VGWI6T')\n",
    "    Discount1 = []\n",
    "    for i in discount1:\n",
    "        Discount1.append(i)\n",
    "\n",
    "    # Scraping next page for more results\n",
    "    driver.get(input(\"Input url here : To scrape page 3\"))\n",
    "\n",
    "    # Creating soup\n",
    "    content = driver.page_source\n",
    "    soup2 = BeautifulSoup(content)\n",
    "    \n",
    "    # Scraping brand of the product - page 3\n",
    "    brand2 = soup2.find_all('div', class_ = '_2B_pmu')\n",
    "    Brand2 = []\n",
    "    for i in brand2:\n",
    "        Brand2.append(i.text[0:40])\n",
    "\n",
    "    # Scraping description of the product - page 3\n",
    "    description2 = soup2.find_all('a', class_ = '_2mylT6')\n",
    "    Product_Description2 = []\n",
    "    for i in description2:\n",
    "        Product_Description2.append(i.text)\n",
    "\n",
    "    # Scraping price of the product - page 3\n",
    "    price2 = soup2.find_all('div', class_ = '_1vC4OE')\n",
    "    Price2 = []\n",
    "    for i in price2:\n",
    "        Price2.append(i.text[0:40])\n",
    "    \n",
    "    # Scraping discounts on the products - page 3\n",
    "    discount2 = soup2.find_all('div', class_ = 'VGWI6T')\n",
    "    Discount2 = []\n",
    "    for i in discount2:\n",
    "        Discount2.append(i)\n",
    "\n",
    "    # Appending lists of all the pages and scrutinizing it appropriately.\n",
    "    BRAND = Brand+Brand1+Brand2\n",
    "    PRODUCT_DESCRIPTION = Product_Description+Product_Description1+Product_Description2\n",
    "    PRICE = Price[0:40]+Price1[0:40]+Price2[0:40]\n",
    "    DISCOUNT = Discount[0:40]+Discount1[0:40]+Discount2[0:40]\n",
    "    \n",
    "    # Creating a dataframe to store the scrapped information\n",
    "    Sunglasses = pd.DataFrame({'BRAND' : BRAND[0:100], 'PRODUCT_DESCRIPTION' : PRODUCT_DESCRIPTION[0:100], 'PRICE' : PRICE[0:100], 'DISCOUNT' : DISCOUNT[0:100]})\n",
    "\n",
    "    # Returning the dataframe\n",
    "    return Sunglasses\n",
    "\n",
    "scrapping()\n",
    "#URL for page 1:\n",
    "#https://www.flipkart.com/search?q=sunglasses&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off\n",
    "\n",
    "#URL for page 2:\n",
    "#https://www.flipkart.com/search?q=sunglasses&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=2\n",
    "\n",
    "#URL for page 3:\n",
    "#https://www.flipkart.com/search?q=sunglasses&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q7: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link: https://www.flipkart.com/apple-iphone-11-black-64-gb-includesearpods-poweradapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the url:https://www.flipkart.com/apple-iphone-11-black-64-gb-includesearpods-poweradapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RATINGS</th>\n",
       "      <th>REVIEW_SUMMARY</th>\n",
       "      <th>FULL_REVIEWS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Amazing phone with great cameras and better ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Great product</td>\n",
       "      <td>Amazing Powerful and Durable Gadget.I’m am ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>It’s a must buy who is looking for an upgrade ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Highly recommended</td>\n",
       "      <td>iphone 11 is a very good phone to buy only if ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Value for money❤️❤️Its awesome mobile phone in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>Best budget Iphone till date ❤️ go for it guys...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>It’s been almost a month since I have been usi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Iphone is just awesome.. battery backup is ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>Nice product</td>\n",
       "      <td>Awesome Phone. Slightly high price but worth. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>Wonderful</td>\n",
       "      <td>*Review after 10 months of usage*Doesn't seem ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  RATINGS      REVIEW_SUMMARY  \\\n",
       "0       5    Perfect product!   \n",
       "1       5       Great product   \n",
       "2       5    Perfect product!   \n",
       "3       5  Highly recommended   \n",
       "4       5    Perfect product!   \n",
       "5       5   Worth every penny   \n",
       "6       5   Worth every penny   \n",
       "7       5    Perfect product!   \n",
       "8       4        Nice product   \n",
       "9       5           Wonderful   \n",
       "\n",
       "                                        FULL_REVIEWS  \n",
       "0  Amazing phone with great cameras and better ba...  \n",
       "1  Amazing Powerful and Durable Gadget.I’m am ver...  \n",
       "2  It’s a must buy who is looking for an upgrade ...  \n",
       "3  iphone 11 is a very good phone to buy only if ...  \n",
       "4  Value for money❤️❤️Its awesome mobile phone in...  \n",
       "5  Best budget Iphone till date ❤️ go for it guys...  \n",
       "6  It’s been almost a month since I have been usi...  \n",
       "7  Iphone is just awesome.. battery backup is ver...  \n",
       "8  Awesome Phone. Slightly high price but worth. ...  \n",
       "9  *Review after 10 months of usage*Doesn't seem ...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def scraping():\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    from selenium import webdriver\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    import requests\n",
    "    import urllib.request\n",
    "    from urllib.request import urlopen\n",
    "    \n",
    "    url=input(\"Enter the url:\")\n",
    "    driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "    driver.get(url)\n",
    "    \n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content)\n",
    "    \n",
    "    rating = soup.find_all('div', class_ = 'hGSR34 E_uFuv')\n",
    "    Rating = []\n",
    "    for i in rating:\n",
    "        Rating.append(i.text[0:100])\n",
    "    \n",
    "    review_summary = soup.find_all('p', class_ = '_2xg6Ul')\n",
    "    Review_Summary = []\n",
    "    for i in review_summary:\n",
    "        Review_Summary.append(i.text[0:100])\n",
    "    \n",
    "    \n",
    "    full_review = soup.find_all('div', class_ = 'qwjRop')\n",
    "    Full_Review = []\n",
    "    for i in full_review:\n",
    "        Full_Review.append(i.text[0:100])\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    Iphone11 = pd.DataFrame({'RATINGS' : Rating[0:100], 'REVIEW_SUMMARY' : Review_Summary[0:100], 'FULL_REVIEWS' : Full_Review[0:100] })\n",
    "    return Iphone11\n",
    "\n",
    "scraping()\n",
    "#https://www.flipkart.com/apple-iphone-11-black-64-gb-includesearpods-poweradapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q8: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the search field.You have to scrape 4 attributes of each sneaker :1. Brand 2. Product Description 3. Price 4. discount %"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "https://www.flipkart.com/search?q=sneakers&as=on&as-show=on&otracker=AS_Query_HistoryAutoSuggest_1_5_na_na_na&otracker1=AS_Query_HistoryAutoSuggest_1_5_na_na_na&as-pos=1&as-type=HISTORY&suggestionId=sneakers&requestId=c2433e4d-d021-4b7c-89be-931cce935fd8&as-searchtext=sneak"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "https://www.flipkart.com/search?q=sneakers&as=on&as-show=on&otracker=AS_Query_HistoryAutoSuggest_1_5_na_na_na&otracker1=AS_Query_HistoryAutoSuggest_1_5_na_na_na&as-pos=1&as-type=HISTORY&suggestionId=sneakers&requestId=c2433e4d-d021-4b7c-89be-931cce935fd8&as-searchtext=sneak&page=2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "https://www.flipkart.com/search?q=sneakers&as=on&as-show=on&otracker=AS_Query_HistoryAutoSuggest_1_5_na_na_na&otracker1=AS_Query_HistoryAutoSuggest_1_5_na_na_na&as-pos=1&as-type=HISTORY&suggestionId=sneakers&requestId=c2433e4d-d021-4b7c-89be-931cce935fd8&as-searchtext=sneak&page=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input url here : To scrape page 1https://www.flipkart.com/search?q=sneakers&as=on&as-show=on&otracker=AS_Query_HistoryAutoSuggest_1_5_na_na_na&otracker1=AS_Query_HistoryAutoSuggest_1_5_na_na_na&as-pos=1&as-type=HISTORY&suggestionId=sneakers&requestId=c2433e4d-d021-4b7c-89be-931cce935fd8&as-searchtext=sneak\n",
      "Input url here : To scrape page 2https://www.flipkart.com/search?q=sneakers&as=on&as-show=on&otracker=AS_Query_HistoryAutoSuggest_1_5_na_na_na&otracker1=AS_Query_HistoryAutoSuggest_1_5_na_na_na&as-pos=1&as-type=HISTORY&suggestionId=sneakers&requestId=c2433e4d-d021-4b7c-89be-931cce935fd8&as-searchtext=sneak&page=2\n",
      "Input url here : To scrape page 3https://www.flipkart.com/search?q=sneakers&as=on&as-show=on&otracker=AS_Query_HistoryAutoSuggest_1_5_na_na_na&otracker1=AS_Query_HistoryAutoSuggest_1_5_na_na_na&as-pos=1&as-type=HISTORY&suggestionId=sneakers&requestId=c2433e4d-d021-4b7c-89be-931cce935fd8&as-searchtext=sneak&page=3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BRAND</th>\n",
       "      <th>PRODUCT_DESCRIPTION</th>\n",
       "      <th>PRICE</th>\n",
       "      <th>DISCOUNT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chevit</td>\n",
       "      <td>Combo Pack of 4 Casual Sneakers With Sneakers ...</td>\n",
       "      <td>₹461</td>\n",
       "      <td>[[76% off]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chevit</td>\n",
       "      <td>171 Smart Tan Lace-Ups Casuals for Men Sneaker...</td>\n",
       "      <td>₹236</td>\n",
       "      <td>[[52% off]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Birde</td>\n",
       "      <td>Combo Pack of 2 Casual Shoes Sneakers For Men</td>\n",
       "      <td>₹569</td>\n",
       "      <td>[[43% off]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chevit</td>\n",
       "      <td>170 Smart Grey Lace-Ups Casuals for Men Sneake...</td>\n",
       "      <td>₹236</td>\n",
       "      <td>[[52% off]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hotstyle</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹189</td>\n",
       "      <td>[[62% off]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Birde</td>\n",
       "      <td>Combo Pack of 4 Casual Shoes Sneakers For Men</td>\n",
       "      <td>₹849</td>\n",
       "      <td>[[57% off]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>World Wear Footwear</td>\n",
       "      <td>Combo Pack of 2 Latest Collection Stylish casu...</td>\n",
       "      <td>₹499</td>\n",
       "      <td>[[50% off]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Bonexy</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹499</td>\n",
       "      <td>[[50% off]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>kruchika</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹298</td>\n",
       "      <td>[[70% off]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>REEBOK</td>\n",
       "      <td>STRIDE RUNNER LP Sneakers For Men</td>\n",
       "      <td>₹1,385</td>\n",
       "      <td>[[34% off]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  BRAND                                PRODUCT_DESCRIPTION  \\\n",
       "0                Chevit  Combo Pack of 4 Casual Sneakers With Sneakers ...   \n",
       "1                Chevit  171 Smart Tan Lace-Ups Casuals for Men Sneaker...   \n",
       "2                 Birde      Combo Pack of 2 Casual Shoes Sneakers For Men   \n",
       "3                Chevit  170 Smart Grey Lace-Ups Casuals for Men Sneake...   \n",
       "4              Hotstyle                                   Sneakers For Men   \n",
       "..                  ...                                                ...   \n",
       "95                Birde      Combo Pack of 4 Casual Shoes Sneakers For Men   \n",
       "96  World Wear Footwear  Combo Pack of 2 Latest Collection Stylish casu...   \n",
       "97               Bonexy                                   Sneakers For Men   \n",
       "98             kruchika                                   Sneakers For Men   \n",
       "99               REEBOK                  STRIDE RUNNER LP Sneakers For Men   \n",
       "\n",
       "     PRICE     DISCOUNT  \n",
       "0     ₹461  [[76% off]]  \n",
       "1     ₹236  [[52% off]]  \n",
       "2     ₹569  [[43% off]]  \n",
       "3     ₹236  [[52% off]]  \n",
       "4     ₹189  [[62% off]]  \n",
       "..     ...          ...  \n",
       "95    ₹849  [[57% off]]  \n",
       "96    ₹499  [[50% off]]  \n",
       "97    ₹499  [[50% off]]  \n",
       "98    ₹298  [[70% off]]  \n",
       "99  ₹1,385  [[34% off]]  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def scrapping():\n",
    "    \n",
    "    # Importing essential libraries\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    from selenium import webdriver\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    import requests\n",
    "    import urllib.request\n",
    "    from urllib.request import urlopen\n",
    "\n",
    "\n",
    "    # Configure the webdriver\n",
    "    driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "    driver.get(input(\"Input url here : To scrape page 1\"))\n",
    "\n",
    "    # Creating soup\n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content)\n",
    "    \n",
    "    brand = soup.find_all('div', class_ = '_2B_pmu')\n",
    "    Brand = []\n",
    "    for i in brand:\n",
    "        Brand.append(i.text[0:40])\n",
    "\n",
    "    # Scraping description of the product\n",
    "    description = soup.find_all('a', class_ = '_2mylT6')\n",
    "    Product_Description = []\n",
    "    for i in description:\n",
    "        Product_Description.append(i.text)\n",
    "\n",
    "    # Scraping price of the product\n",
    "    price = soup.find_all('div', class_ = '_1vC4OE')\n",
    "    Price = []\n",
    "    for i in price:\n",
    "        Price.append(i.text[0:40])\n",
    "\n",
    "    # Scraping discount on the product\n",
    "    discount = soup.find_all('div', class_ = 'VGWI6T')\n",
    "    Discount = []\n",
    "    for i in discount:\n",
    "        Discount.append(i)\n",
    "\n",
    "    \n",
    "    # Scaping next page for more results\n",
    "    driver.get(input(\"Input url here : To scrape page 2\"))\n",
    "\n",
    "    # Creating soup\n",
    "    content = driver.page_source\n",
    "    soup1 = BeautifulSoup(content)\n",
    "    \n",
    "    # Scraping brand of the products - page 2\n",
    "    brand1 = soup1.find_all('div', class_ = '_2B_pmu')\n",
    "    Brand1 = []\n",
    "    for i in brand1:\n",
    "        Brand1.append(i.text[0:40])\n",
    "\n",
    "    # Sctraping description of the products - page 2\n",
    "    description1 = soup1.find_all('a', class_ = '_2mylT6')\n",
    "    Product_Description1 = []\n",
    "    for i in description1:\n",
    "        Product_Description1.append(i.text)\n",
    "\n",
    "    # Scraping price of the products - page 2\n",
    "    price1 = soup1.find_all('div', class_ = '_1vC4OE')\n",
    "    Price1 = []\n",
    "    for i in price1:\n",
    "        Price1.append(i.text[0:40])\n",
    "\n",
    "    # Scraping discount on the products - page 2\n",
    "    discount1 = soup1.find_all('div', class_ = 'VGWI6T')\n",
    "    Discount1 = []\n",
    "    for i in discount1:\n",
    "        Discount1.append(i)\n",
    "\n",
    "    # Scraping next page for more results\n",
    "    driver.get(input(\"Input url here : To scrape page 3\"))\n",
    "\n",
    "    # Creating soup\n",
    "    content = driver.page_source\n",
    "    soup2 = BeautifulSoup(content)\n",
    "    \n",
    "    # Scraping brand of the product - page 3\n",
    "    brand2 = soup2.find_all('div', class_ = '_2B_pmu')\n",
    "    Brand2 = []\n",
    "    for i in brand2:\n",
    "        Brand2.append(i.text[0:40])\n",
    "\n",
    "    # Scraping description of the product - page 3\n",
    "    description2 = soup2.find_all('a', class_ = '_2mylT6')\n",
    "    Product_Description2 = []\n",
    "    for i in description2:\n",
    "        Product_Description2.append(i.text)\n",
    "\n",
    "    # Scraping price of the product - page 3\n",
    "    price2 = soup2.find_all('div', class_ = '_1vC4OE')\n",
    "    Price2 = []\n",
    "    for i in price2:\n",
    "        Price2.append(i.text[0:40])\n",
    "    \n",
    "    # Scraping discounts on the products - page 3\n",
    "    discount2 = soup2.find_all('div', class_ = 'VGWI6T')\n",
    "    Discount2 = []\n",
    "    for i in discount2:\n",
    "        Discount2.append(i)\n",
    "\n",
    "    # Appending lists of all the pages and scrutinizing it appropriately.\n",
    "    BRAND = Brand+Brand1+Brand2\n",
    "    PRODUCT_DESCRIPTION = Product_Description+Product_Description1+Product_Description2\n",
    "    PRICE = Price[0:40]+Price1[0:40]+Price2[0:40]\n",
    "    DISCOUNT = Discount[0:40]+Discount1[0:40]+Discount2[0:40]\n",
    "    \n",
    "    # Creating a dataframe to store the scrapped information\n",
    "    Sunglasses = pd.DataFrame({'BRAND' : BRAND[0:100], 'PRODUCT_DESCRIPTION' : PRODUCT_DESCRIPTION[0:100], 'PRICE' : PRICE[0:100], 'DISCOUNT' : DISCOUNT[0:100]})\n",
    "\n",
    "    # Returning the dataframe\n",
    "    return Sunglasses\n",
    "\n",
    "scrapping()\n",
    "#url1-https://www.flipkart.com/search?q=sneakers&as=on&as-show=on&otracker=AS_Query_HistoryAutoSuggest_1_5_na_na_na&otracker1=AS_Query_HistoryAutoSuggest_1_5_na_na_na&as-pos=1&as-type=HISTORY&suggestionId=sneakers&requestId=c2433e4d-d021-4b7c-89be-931cce935fd8&as-searchtext=sneak\n",
    "#url2-https://www.flipkart.com/search?q=sneakers&as=on&as-show=on&otracker=AS_Query_HistoryAutoSuggest_1_5_na_na_na&otracker1=AS_Query_HistoryAutoSuggest_1_5_na_na_na&as-pos=1&as-type=HISTORY&suggestionId=sneakers&requestId=c2433e4d-d021-4b7c-89be-931cce935fd8&as-searchtext=sneak&page=2\n",
    "#url3-https://www.flipkart.com/search?q=sneakers&as=on&as-show=on&otracker=AS_Query_HistoryAutoSuggest_1_5_na_na_na&otracker1=AS_Query_HistoryAutoSuggest_1_5_na_na_na&as-pos=1&as-type=HISTORY&suggestionId=sneakers&requestId=c2433e4d-d021-4b7c-89be-931cce935fd8&as-searchtext=sneak&page=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q9: Go to the link - https://www.myntra.com/shoes Set Price filter to “Rs. 6649 to Rs. 13099” , Color filter to “Black”, And then scrape First 100 shoes data you get. The data should include “Brand” of the shoes , Short Shoe description, price of the shoe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input url here : To scrape page 1https://www.myntra.com/shoes?f=Color%3ABlack_36454f&rf=Price%3A6683.0_13122.0_6683.0%20TO%2013122.0\n",
      "Input url here : To scrape page 2https://www.myntra.com/shoes?f=Color%3ABlack_36454f&p=2&rf=Price%3A6683.0_13122.0_6683.0%20TO%2013122.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BRAND</th>\n",
       "      <th>PRODUCT_DESCRIPTION</th>\n",
       "      <th>PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ADIDAS</td>\n",
       "      <td>Men FluidFlow Running Shoes</td>\n",
       "      <td>Rs. 7999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADIDAS Originals</td>\n",
       "      <td>Men Solid AR Training Shoes</td>\n",
       "      <td>Rs. 7199Rs. 7999(10% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nike</td>\n",
       "      <td>Men Zoom Running Shoes</td>\n",
       "      <td>Rs. 7995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ADIDAS</td>\n",
       "      <td>Terrex Two Trail Running Shoes</td>\n",
       "      <td>Rs. 9999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ADIDAS</td>\n",
       "      <td>Men Printed SL20 Running Shoes</td>\n",
       "      <td>Rs. 10999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Geox</td>\n",
       "      <td>Men Leather Formal Derbys</td>\n",
       "      <td>Rs. 8999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Cole Haan</td>\n",
       "      <td>Women Oxford Sneakers</td>\n",
       "      <td>Rs. 9799Rs. 13999(30% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Clarks</td>\n",
       "      <td>Men Leather Formal Loafers</td>\n",
       "      <td>Rs. 6999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Geox</td>\n",
       "      <td>Men Leather Formal Slip-Ons</td>\n",
       "      <td>Rs. 12490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Tommy Hilfiger</td>\n",
       "      <td>Men Colourblocked Sneakers</td>\n",
       "      <td>Rs. 9009Rs. 10599(15% OFF)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               BRAND             PRODUCT_DESCRIPTION  \\\n",
       "0             ADIDAS     Men FluidFlow Running Shoes   \n",
       "1   ADIDAS Originals     Men Solid AR Training Shoes   \n",
       "2               Nike          Men Zoom Running Shoes   \n",
       "3             ADIDAS  Terrex Two Trail Running Shoes   \n",
       "4             ADIDAS  Men Printed SL20 Running Shoes   \n",
       "..               ...                             ...   \n",
       "95              Geox       Men Leather Formal Derbys   \n",
       "96         Cole Haan           Women Oxford Sneakers   \n",
       "97            Clarks      Men Leather Formal Loafers   \n",
       "98              Geox     Men Leather Formal Slip-Ons   \n",
       "99    Tommy Hilfiger      Men Colourblocked Sneakers   \n",
       "\n",
       "                         PRICE  \n",
       "0                     Rs. 7999  \n",
       "1    Rs. 7199Rs. 7999(10% OFF)  \n",
       "2                     Rs. 7995  \n",
       "3                     Rs. 9999  \n",
       "4                    Rs. 10999  \n",
       "..                         ...  \n",
       "95                    Rs. 8999  \n",
       "96  Rs. 9799Rs. 13999(30% OFF)  \n",
       "97                    Rs. 6999  \n",
       "98                   Rs. 12490  \n",
       "99  Rs. 9009Rs. 10599(15% OFF)  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def scrapping():\n",
    "    \n",
    "    # Importing essential libraries\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    from selenium import webdriver\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    import requests\n",
    "    import urllib.request\n",
    "    from urllib.request import urlopen\n",
    "\n",
    "\n",
    "    # Configure the webdriver\n",
    "    driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "    driver.get(input(\"Input url here : To scrape page 1\"))\n",
    "\n",
    "    # Creating soup\n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content)\n",
    "    \n",
    "    price = soup.find_all('div', class_ = 'product-price')\n",
    "    Price = []\n",
    "    for i in price:\n",
    "        Price.append(i.text[0:50])\n",
    "\n",
    "    # Scraping description of the product\n",
    "    description = soup.find_all('h4', class_ = 'product-product')\n",
    "    Product_Description = []\n",
    "    for i in description:\n",
    "        Product_Description.append(i.text)\n",
    "\n",
    "    # Scraping price of the product\n",
    "    brand = soup.find_all('h3', class_ = 'product-brand')\n",
    "    Brand = []\n",
    "    for i in brand:\n",
    "        Brand.append(i.text[0:50])\n",
    "\n",
    "   \n",
    "    \n",
    "    # Scaping next page for more results\n",
    "    driver.get(input(\"Input url here : To scrape page 2\"))\n",
    "\n",
    "    # Creating soup\n",
    "    content = driver.page_source\n",
    "    soup1 = BeautifulSoup(content)\n",
    "    \n",
    "    # Scraping brand of the products - page 2\n",
    "    price1 = soup1.find_all('div', class_ = 'product-price')\n",
    "    Price1 = []\n",
    "    for i in price1:\n",
    "        Price1.append(i.text[0:50])\n",
    "\n",
    "    # Sctraping description of the products - page 2\n",
    "    description1 = soup1.find_all('h4', class_ = 'product-product')\n",
    "    Product_Description1 = []\n",
    "    for i in description1:\n",
    "        Product_Description1.append(i.text)\n",
    "\n",
    "    # Scraping price of the products - page 2\n",
    "    brand1 = soup1.find_all('h3', class_ = 'product-brand')\n",
    "    Brand1 = []\n",
    "    for i in brand1:\n",
    "        Brand1.append(i.text[0:50])\n",
    "\n",
    "\n",
    "\n",
    "    # Appending lists of all the pages and scrutinizing it appropriately.\n",
    "    BRAND = Brand+Brand1\n",
    "    PRODUCT_DESCRIPTION = Product_Description+Product_Description1\n",
    "    PRICE = Price[0:50]+Price1[0:50]\n",
    "    \n",
    "    \n",
    "    # Creating a dataframe to store the scrapped information\n",
    "    Shoes = pd.DataFrame({'BRAND' : BRAND[0:100], 'PRODUCT_DESCRIPTION' : PRODUCT_DESCRIPTION[0:100], 'PRICE' : PRICE[0:100]})\n",
    "\n",
    "    # Returning the dataframe\n",
    "    return Shoes\n",
    "\n",
    "scrapping()\n",
    "#url- https://www.myntra.com/shoes?f=Color%3ABlack_36454f&rf=Price%3A6683.0_13122.0_6683.0%20TO%2013122.0\n",
    "#url1- https://www.myntra.com/shoes?f=Color%3ABlack_36454f&p=2&rf=Price%3A6683.0_13122.0_6683.0%20TO%2013122.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q10: Go to webpage https://www.amazon.in/ “Laptop” in the search field and then click the search icon.Then set CPU Type filter to “Intel Core i7” and “Intel Core i9” "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the url:https://www.amazon.in/s?k=Laptop&i=computers&rh=n%3A1375424031%2Cp_n_feature_thirteen_browse-bin%3A12598161031%7C12598163031&dc&qid=1604587547&rnid=12598141031&ref=sr_nr_p_n_feature_thirteen_browse-bin_16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>PRICE</th>\n",
       "      <th>RATING</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dell Inspi</td>\n",
       "      <td>34,831</td>\n",
       "      <td>2.7 out of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dell Inspi</td>\n",
       "      <td>40,490</td>\n",
       "      <td>2.9 out of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HP 15s du2</td>\n",
       "      <td>37,490</td>\n",
       "      <td>3.0 out of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HP 15 10th</td>\n",
       "      <td>35,990</td>\n",
       "      <td>3.6 out of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dell Vostr</td>\n",
       "      <td>36,890</td>\n",
       "      <td>3.5 out of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Dell Inspi</td>\n",
       "      <td>34,831</td>\n",
       "      <td>2.7 out of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HP 15s Thi</td>\n",
       "      <td>35,000</td>\n",
       "      <td>3.9 out of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Lenovo Ide</td>\n",
       "      <td>31,380</td>\n",
       "      <td>5.0 out of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ASUS Exper</td>\n",
       "      <td>41,980</td>\n",
       "      <td>3.7 out of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Dell Inspi</td>\n",
       "      <td>42,100</td>\n",
       "      <td>3.2 out of</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        TITLE   PRICE      RATING\n",
       "0  Dell Inspi  34,831  2.7 out of\n",
       "1  Dell Inspi  40,490  2.9 out of\n",
       "2  HP 15s du2  37,490  3.0 out of\n",
       "3  HP 15 10th  35,990  3.6 out of\n",
       "4  Dell Vostr  36,890  3.5 out of\n",
       "5  Dell Inspi  34,831  2.7 out of\n",
       "6  HP 15s Thi  35,000  3.9 out of\n",
       "7  Lenovo Ide  31,380  5.0 out of\n",
       "8  ASUS Exper  41,980  3.7 out of\n",
       "9  Dell Inspi  42,100  3.2 out of"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def scraping():\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    from selenium import webdriver\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    import requests\n",
    "    import urllib.request\n",
    "    from urllib.request import urlopen\n",
    "    \n",
    "    url=input(\"Enter the url:\")\n",
    "    driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "    driver.get(url)\n",
    "    \n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content)\n",
    "    \n",
    "    title = soup.find_all('span', class_ = 'a-size-medium a-color-base a-text-normal')\n",
    "    Title = []\n",
    "    for i in title:\n",
    "        Title.append(i.text[0:10])\n",
    "    \n",
    "    \n",
    "     \n",
    "    \n",
    "    \n",
    "    price = soup.find_all('span', class_ = 'a-price-whole')\n",
    "    Price = []\n",
    "    for i in price:\n",
    "        Price.append(i.text[0:10])\n",
    "        \n",
    "        \n",
    "    #ratings = soup.find_all('span', class_ ='a-icon-alt')\n",
    "    ratings = soup.find_all('a', class_ ='a-popover-trigger a-declarative')\n",
    "\n",
    "    Ratings = []\n",
    "    for i in ratings:\n",
    "        Ratings.append(i.text[0:10])\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    laptop = pd.DataFrame({'TITLE' : Title[0:10], 'PRICE' : Price[0:10], 'RATING' : Ratings[0:10] })\n",
    "    return laptop\n",
    "\n",
    "scraping()\n",
    "#https://www.amazon.in/s?k=Laptop&i=computers&rh=n%3A1375424031%2Cp_n_feature_thirteen_browse-bin%3A12598161031%7C12598163031&dc&qid=1604587547&rnid=12598141031&ref=sr_nr_p_n_feature_thirteen_browse-bin_16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of assignment-Thank you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
